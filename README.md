nn-from-first-principles

A complete, from-first-principles walkthrough of neural networks â€” starting from the simplest possible model (a single neuron learning Celsius â†’ Fahrenheit) and building up step-by-step to a fully working mini Transformer (GPT-style).

This repository contains 11 hands-on Google Colab notebooks that explain every concept clearly, both mathematically and programmatically, using only NumPy before introducing modern deep learning components.

No frameworks.
No magic.
Just the true mechanics of neural networks.

Table of Contents

About This Project

Notebook Series (11 Parts)

Repository Structure

How to Use

Requirements

License

About This Project

nn-from-first-principles is a guided journey through the foundations of deep learning:

understanding neurons

deriving gradients by hand

implementing backpropagation

building your own autograd engine

training multi-layer perceptrons

processing text with embeddings

implementing self-attention

constructing Transformer blocks

generating text autoregressively

By the end, you'll understand exactly how modern neural networks work under the hood, and youâ€™ll have implemented all core components yourself.

ðŸ”¬ Notebook Series (11 Parts)

Each notebook builds on the last.
Colab links can be added as you upload them.

1. Single Neuron â€” Celsius â†’ Fahrenheit: Fundamentals: linear model, MSE loss, gradients, gradient descent.

2. Multi-Neuron Networks & Activation Functions: Why we need hidden layers, ReLU, tanh, sigmoid.

3. Forward Pass as Matrix Multiplication: Vectorized operations, batched inputs, GPU-friendly math.

4. Backpropagation From Scratch: Full derivation & implementation of backprop for multi-layer networks.

5. Building an Autograd Engine: Implement a minimal PyTorch-like autodiff system.

6. Training Loops & Optimizers: SGD, Momentum, Adam, initialization strategies, LR schedules.

7. MNIST MLP (From Scratch): Build and train a multi-layer perceptron (without deep learning frameworks).

8. Tokenization & Embeddings: Convert text to numbers, build word & positional embeddings.

9. Self-Attention: Implement Q/K/V, scaled dot-product attention, multi-head attention.

10. Transformer Block: LayerNorm, residual connections, feed-forward layers, attention stack.

11. Mini GPT â€” Text Generation: Train a tiny GPT-like model on Shakespeare and generate sequences.

Repository Structure
nn-from-first-principles/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_single_neuron.ipynb
â”‚   â”œâ”€â”€ 02_multi_neuron.ipynb
â”‚   â”œâ”€â”€ 03_forward_pass_matrix.ipynb
â”‚   â”œâ”€â”€ 04_backprop_from_scratch.ipynb
â”‚   â”œâ”€â”€ 05_autograd_engine.ipynb
â”‚   â”œâ”€â”€ 06_training_loops_optimizers.ipynb
â”‚   â”œâ”€â”€ 07_mnist_mlp.ipynb
â”‚   â”œâ”€â”€ 08_tokenization_embeddings.ipynb
â”‚   â”œâ”€â”€ 09_self_attention.ipynb
â”‚   â”œâ”€â”€ 10_transformer_block.ipynb
â”‚   â”œâ”€â”€ 11_mini_gpt.ipynb
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ diagrams/
â”‚   â”œâ”€â”€ plots/
â”‚   â””â”€â”€ samples/
â”‚
â”œâ”€â”€ extras/
â”‚   â”œâ”€â”€ distributed_training.ipynb
â”‚   â”œâ”€â”€ nn_summary_25_pages.ipynb
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ requirements.txt

For Transformer notebooks:

No external deep learning frameworks required (everything is written manually)
